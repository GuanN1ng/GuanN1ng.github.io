---
layout: post 
title:  HBase 架构简述
date:   2023-10-24 22:40:28 
categories: Hbase
---

Apache HBase是一个开源的、分布式的、数据多版本的NoSQL数据库, 项目是Google Bigtable的开源实现，适用于存储半结构化或非结构化的数据，且能够动态扩展列数，能够支撑在数十亿行 X 数百万列的大表上进行随机、实时的
读写访问。底层利用 Hadoop 分布式文件系统（Hadoop Distributed File System，HDFS）提供分布式数据存储。架构图如下：

![Hbase 架构](https://raw.githubusercontent.com/GuanN1ng/diagrams/main/com.guann1n9.diagrams/hbase/hbase_arc.png)

# Zookeeper

Zookeeper在HBase中是沟通一切的桥梁，所有的参与者都和Zookeeper保持心跳会话，并从Zookeeper获取它们需要的集群状态信息，Zookeeper既维护了当前集群状态，也在保证了Hbase的一致性，其作用主要如下：
* 维护当前Hbase集群元信息，包括HMaster、HRegionServer、Namespace、Table等信息；
* 实现**HMaster高可用**，多个HMaster进程只会有一个状态为Active，其余为Backup，通过抢占ZK`/hbase/master`节点实现。若状态为Active的HMaster进程宕机，则所有状态为Backup的HMaster会再次通过抢占ZK`/hbase/master`节点完成选举新的Active HMaster;
* 供Client查询获取HBase元信息。

```shell
ls -R  /hbase
/hbase/master  #active HMaster
/hbase/backup-masters   #所有backup HMaster
/hbase/backup-masters/slave01,16000,1700215955672
/hbase/backup-masters/slave02,16000,1700215956522
/hbase/rs    #所有的RegionServer
/hbase/rs/master,16020,1700215954971
/hbase/rs/slave01,16020,1700215954747
/hbase/rs/slave02,16020,1700215954502
/hbase/namespace  #命名空间
/hbase/namespace/default
/hbase/namespace/hbase
/hbase/namespace/practice
/hbase/table  #集群内的表
/hbase/table/hbase:meta
/hbase/table/hbase:namespace
/hbase/table/practice:diagnose_index
/hbase/table/practice:trace
/hbase/meta-region-server  #负责维护hbase:meta表的RegionServer，存储了集群中所有用户HRegion的位置信息
/hbase/splitWAL
... #未列出剩余节点
```

# HMaster

状态为Active的HMaster负责当前HBase集群的管理工作， HMaster会将管理操作及其运行状态记录到MasterProcWAL中（如服务器崩溃处理、表创建和其他 DDL），通过对MasterProcWAL的维护，实现主节点故障恢复。其主要职责如下：
* 通过监听ZK `rs`节点获取当前RegionServer列表，若有RegionServer节点过期，则处理RegionServer的故障转移；
* 处理用户对元数据变更的请求，如表的创建、修改、删除以及Region的移动、合并、拆分；
* 管理Region的分配与移除，定期的对RegionServer进行负载均衡，调整Region的分配，防止RegionServer数据倾斜过载。


# HRegionServer

RegionServer是HBase最核心的组件，客户端发起的所有数据请求，包括用户数据以及元数据，在完成目标Region定位后，将会直接请求目标Region所在的RegionServer，最后由RegionServer完成数据的读写操作。同时，
RegionServer还将向HMaster（Active）定期汇报自身节点的负载状况，包括RS内存使用状态、在线状态的Region等信息，参与Master的分布式协调管理。

一个RegionServer实例主要包含三部分组件：
* HLog是WAL（Write Ahead Log）预写日志的实现，用于保证Region数据操作的原子性和持久性；
* BlockCache负责缓存从HDFS读取的数据块，提升数据读取性能；
* Region是HBase中实现表的分布式及高可用的基本元素——数据分片，一般有多个Store组成，每Store只负责维护一个列族。若一张表有2个列族，则该表的所有Region都将有2个Store。

## HLog

HLog是WAL的实现，RegionServer处理数据的写请求时，并不会同步的将数据变更写入HDFS中，而是先将Puts和Deletes记录到其HLog中，然后再记录到对应的MemStore中，到达一定的阈值（可配置）后统一Flush到HDFS中，
提高处理效率。只要HLog写入成功，即使RegionServer故障宕机，导致MemStore中未Flush的数据丢失，恢复时仍然可通过重放HLog，完成数据恢复。

除了故障恢复外，HLog还用于主从同步。RegionServer需将HLog中的WALEntry发送给从节点，从节点根据收到的WALEntry执行回放操作，完成主从间的数据复制。


HLog存储于HDFS的`/hbase/WALs/${RegionServer}/`目录中，每隔一段时间（由参数`hbase.regionserver.logroll.period`决定，默认1小时）RegionServer会创建一个新的日志文件，接收后续的日志数据，完成HLog的滚动。

```shell
[root@master bin]# ./hdfs  dfs  -ls  /hbase/WALs/
Found 3 items
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:18 /hbase/WALs/master,16020,1700215954971
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/slave01,16020,1700215954747
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/slave02,16020,1700215954502
[root@master bin]#
[root@master bin]# ./hdfs  dfs  -ls  /hbase/WALs/master,16020,1700215954971/
Found 2 items
-rw-r--r--   3 hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/master,16020,1700215954971/master%2C16020%2C1700215954971.1700622799415
-rw-r--r--   3 hadoop supergroup          0 2023-11-22 11:18 /hbase/WALs/master,16020,1700215954971/master%2C16020%2C1700215954971.meta.1700623109490.meta
```



当**某一段HLog中记录的数据变动已全部从MemStore Flush到HDFS中，则该段HLog则被认为已失效**。RegionServer会将已失效的HLog日志片段从`/hbase/WALs/${RegionServer}/`移动至`/hbase/oldWALs/`目录中，HMaster节点将
定期（`hbase.master.cleaner.interval`）扫描`/hbase/oldWALs/`目录，将确认可删除的日志文件删除。




```shell
[root@master bin]# ./hdfs  dfs  -ls  /hbase/oldWALs
Found 458 items
-rw-r--r--   3 hadoop supergroup      42785 2023-11-17 18:27 /hbase/oldWALs/master%2C16000%2C1700215954171.1700215964291$masterlocalwal$
-rw-r--r--   3 hadoop supergroup       1009 2023-11-17 18:42 /hbase/oldWALs/master%2C16000%2C1700215954171.1700216870147$masterlocalwal$
-rw-r--r--   3 hadoop supergroup         93 2023-11-17 18:57 /hbase/oldWALs/master%2C16000%2C1700215954171.1700217770241$masterlocalwal$
...
```



## BlockCache

RegionServer通过维护BlockCache和Bloom Filter以实现查询优化。HBase内部提供了两种不同的BlockCache实现来缓存从HDFS读取的数据： 默认的on-heap LruBlockCache和BucketCache（通常是off-heap）。
除此外，还可使用**基于外部系统memcached实现的MemcachedBlockCache**，但**需要确保RegionServer与memcached的网络良好，否则将极大地拖慢HBase的处理速度**，一般不建议使用。

### LruBlockCache

LruBlockCache是LRU（Least Recently Used，最近最少使用）缓存的实现，按照被回收的优先级可分为三部分：
* Single access priority：单次访问区，占比25%，当一个数据块第一次被从HDFS读取后，此时对应的优先级即为Single access priority，当LRU缓存空间不足需要回收时，Single access priority的对象将被优先考虑；
* Multi access priority：多次访问区，占比50%，当一个数据块，属于Single Access优先级，但是之后被再次访问，则它会升级为Multi Access优先级。在缓存空间进行回收时，这部分内容属于次要被考虑的范围；
* In-memory access priority：占比25%，与数据块被访问的次数无关，如果数据块存储的列族被配置为"in-memory"，则该数据块的优先级即为In-memory access priority。在缓存空间进行回收时，这部分内容属于最后被考虑的范围。

列族设置为"in-memory"的方法为：
* 建表语句：`create 't', {NANME => 'f', IN_MEMORY => 'true'}`
* Java api：`HColumnDescriptor.setInMemory(true)`

LruBlockCache可用内存大小为：

```
RegionServer堆大小 * hfile.block.cache.size * 0.99
```
hfile.block.cache.size配置默认为0.4，一般不建议调整，HBase使用时需确保`hbase.regionserver.global.memstore.size + hfile.block.cache.size <= 0.8`，否则将报错。

LruBlockCache是完全基于JVM Heap的缓存，随着内存中对象越来越多，每隔一段时间都会引发一次Full GC。在Full GC的过程中，整个JVM完全处于停滞状态，降低HBase的可用性。


### BucketCache

BucketCache提供了三种可供使用的存储策略：


## HRegion

Region是HBase中实现表的分布式及高可用的基本元素——数据分片，负责维护对应表在该Region上的每个列族（Column Family）的存储（Store），列族与Store一对一，表定义中有几个列族，则对应的Region中有几个Store。
对象的层次结构如下：
```
Table                    (HBase table)
    Region               (Regions for the table)
        Store            (Store per ColumnFamily for each Region for the table)
            MemStore     (MemStore for each Store for each Region for the table)
            StoreFile    (StoreFiles for each Store for each Region for the table)
                Block    (Blocks within a StoreFile within a Store for each Region for the table)

```

Region的分配由HMaster负责，分配的结果则存储在`hbase:meta`表中，Hbase被设计为每个RegionServer实例运行时，维护一个较低的Region数量（20~200），同时单个Region管理的Store数据大小（HDFS文件）在5~20GB间。


### HStore


#### MemStore


#### HFile
