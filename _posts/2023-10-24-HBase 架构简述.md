---
layout: post 
title:  HBase 架构简述
date:   2023-10-24 22:40:28 
categories: Hbase
---

Apache HBase是一个开源的、分布式的、数据多版本的NoSQL数据库, 项目是Google Bigtable的开源实现，适用于存储半结构化或非结构化的数据，且能够动态扩展列数，能够支撑在数十亿行 X 数百万列的大表上进行随机、实时的
读写访问。底层利用 Hadoop 分布式文件系统（Hadoop Distributed File System，HDFS）提供分布式数据存储。架构图如下：

![Hbase 架构](https://raw.githubusercontent.com/GuanN1ng/diagrams/main/com.guann1n9.diagrams/hbase/hbase_arc.png)

# Zookeeper

Zookeeper在HBase中是沟通一切的桥梁，所有的参与者都和Zookeeper保持心跳会话，并从Zookeeper获取它们需要的集群状态信息，Zookeeper既维护了当前集群状态，也在保证了Hbase的一致性，其作用主要如下：
* 维护当前Hbase集群元信息，包括HMaster、HRegionServer、Namespace、Table等信息；
* 实现**HMaster高可用**，多个HMaster进程只会有一个状态为Active，其余为Backup，通过抢占ZK`/hbase/master`节点实现。若状态为Active的HMaster进程宕机，则所有状态为Backup的HMaster会再次通过抢占ZK`/hbase/master`节点完成选举新的Active HMaster;
* 供Client查询获取HBase元信息。

```shell
ls -R  /hbase
/hbase/master  #active HMaster
/hbase/backup-masters   #所有backup HMaster
/hbase/backup-masters/slave01,16000,1700215955672
/hbase/backup-masters/slave02,16000,1700215956522
/hbase/rs    #所有的RegionServer
/hbase/rs/master,16020,1700215954971
/hbase/rs/slave01,16020,1700215954747
/hbase/rs/slave02,16020,1700215954502
/hbase/namespace  #命名空间
/hbase/namespace/default
/hbase/namespace/hbase
/hbase/namespace/practice
/hbase/table  #集群内的表
/hbase/table/hbase:meta
/hbase/table/hbase:namespace
/hbase/table/practice:diagnose_index
/hbase/table/practice:trace
/hbase/meta-region-server  #负责维护hbase:meta表的RegionServer，存储了集群中所有用户HRegion的位置信息
/hbase/splitWAL
... #未列出剩余节点
```

# HMaster

状态为Active的HMaster负责当前HBase集群的管理工作， HMaster会将管理操作及其运行状态记录到MasterProcWAL中（如服务器崩溃处理、表创建和其他 DDL），通过对MasterProcWAL的维护，实现主节点故障恢复。其主要职责如下：
* 通过监听ZK `rs`节点获取当前RegionServer列表，若有RegionServer节点过期，则处理RegionServer的故障转移；
* 处理用户对元数据变更的请求，如表的创建、修改、删除以及Region的移动、合并、拆分；
* 管理Region的分配与移除，定期的对RegionServer进行负载均衡，调整Region的分配，防止RegionServer数据倾斜过载。


# HRegionServer

RegionServer是HBase最核心的组件，客户端发起的所有数据请求，包括用户数据以及元数据，在完成目标Region定位后，将会直接请求目标Region所在的RegionServer，最后由RegionServer完成数据的读写操作。同时，
RegionServer还将向HMaster（Active）定期汇报自身节点的负载状况，包括RS内存使用状态、在线状态的Region等信息，参与Master的分布式协调管理。

一个RegionServer实例主要包含三部分组件：
* HLog是WAL（Write Ahead Log）预写日志的实现，用于保证Region数据操作的原子性和持久性；
* BlockCache负责缓存从HDFS读取的数据块，提升数据读取性能；
* Region是HBase中实现表的分布式及高可用的基本元素——数据分片，一般有多个Store组成，每Store只负责维护一个列族。若一张表有2个列族，则该表的所有Region都将有2个Store。

## HLog

HLog是WAL的实现，RegionServer处理数据的写请求时，并不会同步的将数据变更写入HDFS中，而是先将Puts和Deletes记录到其HLog中，然后再记录到对应的MemStore中，到达一定的阈值（可配置）后统一Flush到HDFS中，
提高处理效率。只要HLog写入成功，即使RegionServer故障宕机，导致MemStore中未Flush的数据丢失，恢复时仍然可通过重放HLog，完成数据恢复。

除了故障恢复外，HLog还用于主从同步。RegionServer需将HLog中的WALEntry发送给从节点，从节点根据收到的WALEntry执行回放操作，完成主从间的数据复制。


HLog存储于HDFS的`/hbase/WALs/${RegionServer}/`目录中，每隔一段时间（由参数`hbase.regionserver.logroll.period`决定，默认1小时）RegionServer会创建一个新的日志文件，接收后续的日志数据，完成HLog的滚动。

```shell
[root@master bin]# ./hdfs  dfs  -ls  /hbase/WALs/
Found 3 items
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:18 /hbase/WALs/master,16020,1700215954971
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/slave01,16020,1700215954747
drwxr-xr-x   - hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/slave02,16020,1700215954502
[root@master bin]#
[root@master bin]# ./hdfs  dfs  -ls  /hbase/WALs/master,16020,1700215954971/
Found 2 items
-rw-r--r--   3 hadoop supergroup          0 2023-11-22 11:13 /hbase/WALs/master,16020,1700215954971/master%2C16020%2C1700215954971.1700622799415
-rw-r--r--   3 hadoop supergroup          0 2023-11-22 11:18 /hbase/WALs/master,16020,1700215954971/master%2C16020%2C1700215954971.meta.1700623109490.meta
```



当**某一段HLog中记录的数据变动已全部从MemStore Flush到HDFS中，则该段HLog则被认为已失效**。RegionServer会将已失效的HLog日志片段从`/hbase/WALs/${RegionServer}/`移动至`/hbase/oldWALs/`目录中，HMaster节点将
定期（`hbase.master.cleaner.interval`）扫描`/hbase/oldWALs/`目录，将确认可删除的日志文件删除。




```shell
[root@master bin]# ./hdfs  dfs  -ls  /hbase/oldWALs
Found 458 items
-rw-r--r--   3 hadoop supergroup      42785 2023-11-17 18:27 /hbase/oldWALs/master%2C16000%2C1700215954171.1700215964291$masterlocalwal$
-rw-r--r--   3 hadoop supergroup       1009 2023-11-17 18:42 /hbase/oldWALs/master%2C16000%2C1700215954171.1700216870147$masterlocalwal$
-rw-r--r--   3 hadoop supergroup         93 2023-11-17 18:57 /hbase/oldWALs/master%2C16000%2C1700215954171.1700217770241$masterlocalwal$
...
```



## BlockCache

RegionServer通过维护BlockCache和Bloom Filter以实现查询优化。HBase内部提供了两种不同的BlockCache实现来缓存从HDFS读取的数据： 默认的on-heap LruBlockCache和BucketCache（通常是off-heap）。
除此外，还可使用**基于外部系统memcached实现的MemcachedBlockCache**，但**需要确保RegionServer与memcached的网络良好，否则将极大地拖慢HBase的处理速度**，一般不建议使用。

### LruBlockCache

LruBlockCache是LRU（Least Recently Used，最近最少使用）缓存的实现，按照被回收的优先级可分为三部分：
* Single access priority：单次访问区，占比25%，当一个数据块第一次被从HDFS读取后，此时对应的优先级即为Single access priority，当LRU缓存空间不足需要回收时，Single access priority的对象将被优先考虑；
* Multi access priority：多次访问区，占比50%，当一个数据块，属于Single Access优先级，但是之后被再次访问，则它会升级为Multi Access优先级。在缓存空间进行回收时，这部分内容属于次要被考虑的范围；
* In-memory access priority：占比25%，与数据块被访问的次数无关，如果数据块存储的列族被配置为"in-memory"，则该数据块的优先级即为In-memory access priority。在缓存空间进行回收时，这部分内容属于最后被考虑的范围，如`hbase:meta`表。

列族设置为"in-memory"的方法为：
* 建表语句：`create 't', {NANME => 'f', IN_MEMORY => 'true'}`
* Java api：`HColumnDescriptor.setInMemory(true)`

LruBlockCache可用内存大小为：

```
RegionServer堆大小 * hfile.block.cache.size * 0.99
```
hfile.block.cache.size配置默认为0.4，一般不建议调整，HBase使用时需确保`hbase.regionserver.global.memstore.size + hfile.block.cache.size <= 0.8`，否则将报错。

LruBlockCache是完全基于JVM Heap的缓存，随着内存中对象越来越多，每隔一段时间都会引发一次Full GC。在Full GC的过程中，整个JVM完全处于停滞状态，降低HBase的可用性。


### BucketCache

BucketCache主要设计目的是将用户数据块缓存从JVM堆中移出，共有三种模式可供选择：off-heap（直接内存）、file（文件）以及 [mmaped file](https://en.wikipedia.org/wiki/Memory-mapped_file)（内存映射文件），通过`hbase.bucketcache.ioengine`进行配置。

具体使用时是通过CombinedBlockCache进行管理，CombinedBlockCache同时维护了LruBlockCache和BucketCache，LruBlockCache仅负责用于缓存元数据，如索引及Bloom Filter数据，其余数据块则使用BucketCache缓存。


## HRegion

Region是HBase中实现表的分布式及高可用的基本元素——数据分片，负责维护对应表在该Region上的每个列族（Column Family）的存储（Store），列族与Store一对一，表定义中有几个列族，则对应的Region中有几个Store。
对象的层次结构如下：
```
Table                    (HBase table)
    Region               (Regions for the table)
        Store            (Store per ColumnFamily for each Region for the table)
            MemStore     (MemStore for each Store for each Region for the table)
            StoreFile    (StoreFiles for each Store for each Region for the table)
                Block    (Blocks within a StoreFile within a Store for each Region for the table)

```

Region的分配由HMaster负责，分配的结果则存储在`hbase:meta`表中，Hbase被设计为每个RegionServer实例运行时，维护一个较低的Region数量（20~200），同时单个Region管理的Store数据大小（HDFS文件）在5~20GB间。原因如下：
* 每个MemStore初始化时默认需要2MB（`hbase.hregion.memstore.mslab.chunksize`）的堆内存缓冲区，过多的Region会过度占用JVM堆，例表有2个列族，则每个Region有2个Store，若RegionServer维护了1000个Region，**即使此时还没有数据，仅MemStore初始化就会占用1000 * 2 * 2MB共约4GB的堆内存**。
* 过多的Region可会会频繁的触发MemStore的Flush以及相应HFile的合并，导致HBase性能降低。

### HStore

Store在Region中负责维护一个列族，由一个MemStore和0~n个HFile组成。

#### MemStore


MemStore用于缓存数据修改，通过与HLog搭配使用，提高数据的写入效率。除去手动通过API或命令触发Flush，当到达一定阈值后，HBase也会自动触发MemStore的Flush，将内存中的快照同步到文件中，触发时机主要有如下4种级别：

* 定时FLush：通过参数`hbase.regionserver.optionalcacheflushinterval`参数控制，默认3600000ms，即1h，**可设置为0，关闭自动刷新**；
* MemStore级别：单个MemStore的大小达到配置阈值`hbase.hregion.memstore.flush.size`（默认128M）则会触发FLush;
* Region级别：Region中所有MemStore的大小总和达到了配置阈值`hbase.hregion.memstore.block.multiplier *  hbase.hregion.memstore.flush.size`（默认 4*128M = 512M）则会触发FLush;
* RegionServer级别：
  * RegionServer内所有MemStore的大小超过`hbase.regionserver.global.memstore.lowerLimit`（默认95% of `hbase.regionserver.global.memstore.size`）配置值，
    开始执行Flush，Flush时，按Region为单位，并以Region中MemStore的总和大小降序执行。
  * RegionServer内所有MemStore的大小达到`hbase.regionserver.global.memstore.upperLimit`（默认40% of heap）指定的值时，RegionServer会**阻塞更新并强制执行Flush**，
    直到总MemStore大小低于`hbase.regionserver.global.memstore.lowerLimit`配置值。
  * RegionServer内HLog日志条目数达到`hbase.regionserver.max.logs`中指定的值时，MemStore将被刷新到HFile以减少HLog中的日志数量。FLush顺序基于HLog中最早日志对应的MemStore，
    直到HLog数量下降到`hbase.regionserver.max.logs`以下。
  


Region中任一的MemStore发生Flush时，该Region上所有的MemStore也都将会被Flush。若表有多个列族，且列族与列族件数据不均衡，数据写入频繁的MemStore Flush时，也会导致
其它列族的MemStore Flush，造成性能浪费，**建议表设计时，不要定义过多的列族**。



#### HFile

HFile对象是对HDFS的文件映射，即数据的物理存储。存储目录为`/hbase/data/${namespace}/${tableName}/${region}/${columnName}/${HFile文件}`，示例如下：

```shell
[root@master bin]# ./hdfs  dfs  -ls /hbase/data/practice/trace/    
Found 2 items
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 19:56 /hbase/data/practice/trace/.tabledesc
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 21:05 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd
[root@master bin]#
[root@master bin]# ./hdfs  dfs  -ls /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd   
Found 5 items
-rw-r--r--   3 hadoop supergroup         41 2023-11-21 19:56 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/.regioninfo
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 21:05 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/.tmp
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 21:05 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/f1
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 19:56 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/f2
drwxr-xr-x   - hadoop supergroup          0 2023-11-21 19:57 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/recovered.edits
[root@master bin]#
[root@master bin]# ./hdfs  dfs  -ls /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/f1/
Found 1 items
-rw-r--r--   3 hadoop supergroup       5031 2023-11-21 21:05 /hbase/data/practice/trace/dc04ce3452ce6ea3821b02756b9acfdd/f1/ab72c4203e0a4190a69be728613706f5
```

随着MemStore不断地Flush，会不断生成对应的HFile，越来越多的文件会导致查询IO增加，降低查询效率。此时，则需要对HFile进行合并，HBase根据合并规模将Compaction分为了两类：
* Minor Compaction：指选取部分小的、相邻的 HFile，将它们合并成一个更大的HFile；
* Major Compaction：将一个Store中所有的HFile合并成一个HFile，这个过程会清理三种无意义的数据：TTL过期数据、被删除的数据与版本号超过设定版本号的数据。
