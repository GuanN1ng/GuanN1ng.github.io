---
title:  Kafka 消息发送
date:   2021-08-30 21:33:42
categories: Kafka
---

主线程将需发送的消息写入AccumulatorRecord中后，后续将消息发送至Kafka Broker的工作主要涉及到2个Kafka组件：**Sender线程及NetworkClient网络客户端**。
Sender线程负责将AccumulatorRecord转化为网络请求的ClientRequest对象以及等幂、事务等业务处理，而NetworkClient负责具体的发送实现。


### Sender初始化

Sender线程负责从RecordAccumulator中拉取消息，并通过网络IO发送至Kafka Broker中。Sender在创建Kafka Producer时完成初始化并启动：

KafkaProducer#KafkaProducer

```
this.sender = newSender(logContext, kafkaClient, this.metadata);
String ioThreadName = NETWORK_THREAD_PREFIX + " | " + clientId;
this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
this.ioThread.start();
```

Sender线程为**KafkaThread类的实现，会被设置为守护线程**，KafkaThread继承自Java Thread：

```
public KafkaThread(final String name, Runnable runnable, boolean daemon) {
    super(runnable, name);
    configureThread(name, daemon);
}

private void configureThread(final String name, boolean daemon) {
    setDaemon(daemon);
    setUncaughtExceptionHandler((t, e) -> log.error("Uncaught exception in thread '{}':", name, e));
}
```

### Run方法

run方法中通过调用while循环调用runOnce方法实现消息发送(runOnce方法中还包含了事务处理，后续更新)：

```
void runOnce() {
        //事务处理
        if (transactionManager != null) {...}

        long currentTimeMs = time.milliseconds();
        long pollTimeout = sendProducerData(currentTimeMs);
        client.poll(pollTimeout, currentTimeMs);
    }
```

#### sendProducerData

对于KafkaProducer的应用逻辑来说，需要关注消息是发向哪个主题分区，但对于网络连接来说，客户端需要关注的是与哪个Broker建立连接，不关心系消息属于哪个分区，Sender线程
需要将<TopicPartition,Deque<ProducerBatch>>的数据结构转变为<NodeId,List<ProducerBatch>>的形式，NodeId即为Kafka Broker的id,最终封装成为ClientRequest对象。

* 1、检查当前有消息待发送的节点，并更新元数据

```
Cluster cluster = metadata.fetch(); //获取本地元数据
ReadyCheckResult result = this.accumulator.ready(cluster, now); //获取待发送节点数据
if (!result.unknownLeaderTopics.isEmpty()) {
    //有未知leader分区的Topic,再次请求获取元数据
    for (String topic : result.unknownLeaderTopics)
        this.metadata.add(topic, now);
    log.debug("Requesting metadata update due to unknown leader topics from the batched records: {}",
        result.unknownLeaderTopics);
    this.metadata.requestUpdate();
}
```

* 2、判断与Broker节点的IO连接是否可用，不可用，移除节点。

```
Iterator<Node> iter = result.readyNodes.iterator();
long notReadyTimeout = Long.MAX_VALUE;
while (iter.hasNext()) {
    Node node = iter.next();
    if (!this.client.ready(node, now)) {  //判断连接是否可用
        iter.remove();
        notReadyTimeout = Math.min(notReadyTimeout, this.client.pollDelayMs(node, now));
    }
}
```

* 3、从RecordAccumulator中拉取消息，按照BrokerId分类，并判断是否保证发送顺序，**max.in.flight.requests.per.connection参数为1，表示需要保证分区发送顺序，则调用
accumulator.mutePartition(batch.topicPartition),对当前分区加锁**。

```
Map<Integer, List<ProducerBatch>> batches = accumulator.drain(cluster, result.readyNodes, this.maxRequestSize, now);
addToInflightBatches(batches);
if (guaranteeMessageOrder) {
    // 确保分区消息有序   maxInflightRequests == 1
    for (List<org.apache.kafka.clients.producer.internals.ProducerBatch> batchList : batches.values()) {
        for (org.apache.kafka.clients.producer.internals.ProducerBatch batch : batchList)
            this.accumulator.mutePartition(batch.topicPartition); //对当前分区加锁
    }
}
```

* 4、处理超时消息，移除超时未发送消息，delivery.timeout.ms参数，默认时间是120s

```
accumulator.resetNextBatchExpiryTime();
List<ProducerBatch> expiredInflightBatches = getExpiredInflightBatches(now);
List<ProducerBatch> expiredBatches = this.accumulator.expiredBatches(now);
expiredBatches.addAll(expiredInflightBatches);
if (!expiredBatches.isEmpty())
    log.trace("Expired {} batches in accumulator", expiredBatches.size());
for (org.apache.kafka.clients.producer.internals.ProducerBatch expiredBatch : expiredBatches) {
    String errorMessage = "Expiring " + expiredBatch.recordCount + " record(s) for " + expiredBatch.topicPartition
        + ":" + (now - expiredBatch.createdMs) + " ms has passed since batch creation";
    failBatch(expiredBatch, new TimeoutException(errorMessage), false);
    if (transactionManager != null && expiredBatch.inRetry()) {
        transactionManager.markSequenceUnresolved(expiredBatch);
    }
}
sensors.updateProduceRequestMetrics(batches);
```

* 5、注册结果回调处理函数，并完成ClientRequest的构造

```
RequestCompletionHandler callback = response -> handleProduceResponse(response, recordsByPartition, time.milliseconds());
String nodeId = Integer.toString(destination);
ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0,
        requestTimeoutMs, callback);
client.send(clientRequest, now);
```

### NetworkClient

NetworkClient对Kafka对网络层的封装实现，底层采用Java NIO类库实现，添加了一些额外的功能，具体关系如下：

![kafka NIO](https://raw.githubusercontent.com/GuanN1ng/diagrams/main/com.guann1n9.diagrams/kakfa/networkclient.png)


* NetworkSend 数据发送Buffer
* NetworkReceive 接收数据Buffer
* TransportLayer  SocketChannel的封装
* Kafka Selector持有Java NIO中Selector类型的成员变量，以及所有的KafkaChannel

```
private final java.nio.channels.Selector nioSelector;
private final Map<String, KafkaChannel> channels;
```

NetworkClient的消息发送通过两个核心方法完成：

```
public void send(ClientRequest request, long now)
public List<ClientResponse> poll(long timeout, long now)
```

#### NetworkClient#send

send方法主要是完成网络IO前的最后一步：

```
inFlightRequests.add(inFlightRequest);
selector.send(new NetworkSend(clientRequest.destination(), send));
```

* 将请求保存到InFlightRequests中，InFlightRequests中保存着准备发送或等待响应的请求，以便后续进行重试。
* 调用Selector#send，将数据写入对应KafkaChannel的NetworkSend(Buffer)中，并监听TransportLayer(SocketChannel)对应的写事件。

#### NetworkClient#poll

poll中，调用Selector#poll方法，并完成Selector中所有Channel的事件。

```
try {
    this.selector.poll(Utils.min(timeout, metadataTimeout, defaultRequestTimeoutMs));
} catch (IOException e) {
    log.error("Unexpected error during I/O", e);
}

// process completed actions
long updatedNow = this.time.milliseconds();
List<ClientResponse> responses = new ArrayList<>();
handleCompletedSends(responses, updatedNow);
handleCompletedReceives(responses, updatedNow);
handleDisconnections(responses, updatedNow);
handleConnections();
handleInitiateApiVersionRequests(updatedNow);
handleTimedOutConnections(responses, updatedNow);
handleTimedOutRequests(responses, updatedNow);
completeResponses(responses);
```


* completeResponses

此方法内调用Sender线程设置的回调函数RequestCompletionHandler->Sender#completeBatch()，消息重试、ProducerBatch清理及事务处理。

```
if (error == Errors.MESSAGE_TOO_LARGE && batch.recordCount > 1 && !batch.isDone() &&
        (batch.magic() >= RecordBatch.MAGIC_VALUE_V2 || batch.isCompressed())) {
    //消息太大，分割再次尝试发送，不占用重试次数
    if (transactionManager != null)
        transactionManager.removeInFlightBatch(batch);
    this.accumulator.splitAndReenqueue(batch);
    maybeRemoveAndDeallocateBatch(batch);
    this.sensors.recordBatchSplit();
} else if (error != Errors.NONE) {
    //可重试的异常
    if (canRetry(batch, response, now)) {
        reenqueueBatch(batch, now); //重试
    } else if (error == Errors.DUPLICATE_SEQUENCE_NUMBER) {
        //重试机制导致发送出去重复的消息  SEQUENCE_NUMBER 幂等时的序列号
        completeBatch(batch, response);
    } else {
        //
        failBatch(batch, response, batch.attempts() < this.retries);
    }
    if (error.exception() instanceof InvalidMetadataException) {
        //更新元数据
        metadata.requestUpdate();
    }
} else {
    completeBatch(batch, response);
}

// Unmute the completed partition.
if (guaranteeMessageOrder)
    this.accumulator.unmutePartition(batch.topicPartition);

```

需重试的ProducerBatch的添加到队列头部，以尽快完成重试。